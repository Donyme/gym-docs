I"#<p>The agent (a car) is started at the bottom of a valley. For any given state
the agent may choose to accelerate to the left, right or cease any
acceleration. The code is originally based on <a href="http://incompleteideas.net/MountainCar/MountainCar1.cp">this code</a>
and the environment appeared first in Andrew Moore’s PhD Thesis (1990):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@TECHREPORT{Moore90efficientmemory-based,
    author = {Andrew William Moore},
    title = {Efficient Memory-based Learning for Robot Control},
    institution = {},
    year = {1990}
}
</code></pre></div></div>

<h2 id="observation-space">Observation Space</h2>

<p>The observation space is a 2-dim vector, where the 1st element represents the “car position” and the 2nd element represents the “car velocity”.</p>

<h2 id="action">Action</h2>

<p>The actual driving force is calculated by multiplying the power coef by power (0.0015)</p>

<h2 id="reward">Reward</h2>

<p>Reward of 100 is awarded if the agent reached the flag (position = 0.45)
on top of the mountain. Reward is decrease based on amount of energy consumed each step.</p>

<h2 id="starting-state">Starting State</h2>

<p>The position of the car is assigned a uniform random value in [-0.6 , -0.4]. The starting velocity of the car is always assigned to 0.</p>

<h2 id="episode-termination">Episode Termination</h2>

<p>The car position is more than 0.45. Episode length is greater than 200</p>

<h2 id="arguments">Arguments</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gym.make('MountainCarContinuous-v0')
</code></pre></div></div>

<h2 id="version-history">Version History</h2>

<ul>
  <li>v0: Initial versions release (1.0.0)</li>
</ul>
:ET