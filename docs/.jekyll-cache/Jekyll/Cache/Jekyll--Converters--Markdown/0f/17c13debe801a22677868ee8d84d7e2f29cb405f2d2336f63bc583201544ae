I"ø<p>The agent (a car) is started at the bottom of a valley. For any given state
the agent may choose to accelerate to the left, right or cease any
acceleration. The code is originally based on <a href="http://incompleteideas.net/MountainCar/MountainCar1.cp">this code</a>
and the environment appeared first in Andrew Moore‚Äôs PhD Thesis (1990):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@TECHREPORT{Moore90efficientmemory-based,
    author = {Andrew William Moore},
    title = {Efficient Memory-based Learning for Robot Control},
    institution = {},
    year = {1990}
}
</code></pre></div></div>

<p>Observation space is a 2-dim vector, where the 1st element represents the ‚Äúcar position‚Äù and the 2nd element represents the ‚Äúcar velocity‚Äù.</p>

<p>There are 3 discrete deterministic actions:</p>
<ul>
  <li>0: Accelerate to the Left</li>
  <li>1: Don‚Äôt accelerate</li>
  <li>2: Accelerate to the Right</li>
</ul>

<p>Reward: Reward of 0 is awarded if the agent reached the flag
(position = 0.5) on top of the mountain. Reward of -1 is awarded if the position of the agent is less than 0.5.</p>

<p>Starting State: The position of the car is assigned a uniform random value in [-0.6 , -0.4]. The starting velocity of the car is always assigned to 0.</p>

<p>Episode Termination: The car position is more than 0.5. Episode length is greater than 200</p>

<h3 id="arguments">Arguments</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gym.make('MountainCar-v0')
</code></pre></div></div>

<h3 id="version-history">Version History</h3>

<ul>
  <li>v0: Initial versions release (1.0.0)</li>
</ul>
:ET